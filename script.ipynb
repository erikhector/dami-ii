import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import f1_score

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

from xgboost import XGBClassifier

df = pd.read_excel(r'/default_of_credit_card_clients.xlsx')
df.head()

#Info 
df.info()

df = df.drop('ID', axis=1)

# Descriptions
df.columns
df.describe()

X = df[['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',
       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',
       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',
       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]
y = df['default payment next month']  


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=42, stratify=y)

count_0_test = 0
count_1_test = 0
for line in y_test:
  if line == 0:
    count_0_test += 1
  if line == 1:
    count_1_test += 1

count_0_train = 0
count_1_train = 0
for line in y_train:
  if line == 0:
    count_0_train += 1
  if line == 1:
    count_1_train += 1

percentage_test = round((count_1_test / count_0_test) * 100, 2)
percentage_train = round((count_1_train / count_0_train) * 100, 2)

# Print distribution
print("class distribution in train_set " + str(percentage_train) + "% default(" + str(count_1_train) + ") " + str(100 - percentage_train) + "% not default(" + str(count_0_train) + ")" + " total: " + str(count_1_train + count_0_train))
print("class distribution in test_set " + str(percentage_test) + "% default(" + str(count_1_test) + ") " + str(100 - percentage_test) + "% not default(" + str(count_0_test) + ")" + " total: " + str(count_1_test + count_0_test))


# Initial evaluation of models. Any models can be added here. no hyperp done here
models = []
models.append(('LR', LogisticRegression(solver='saga', multi_class='multinomial', max_iter=200)))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('RFC', RandomForestClassifier()))
models.append(('XGB', XGBClassifier(early_stopping_rounds=10)))
models.append(('GBC', GradientBoostingClassifier()))
models.append(('NB', GaussianNB()))

# # evaluate each model in turn
results = []
names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)
    cv_results = cross_val_score(model, X_train, y_train.values.ravel(), cv=kfold, scoring='roc_auc',n_jobs=-1, verbose=True)
    results.append(cv_results)
    names.append(name)
    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
    
    clf = StackingClassifier(estimators=models, final_estimator=LogisticRegression())

kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)
cv_results = cross_val_score(clf, X_train, y_train.values.ravel(), cv=kfold, scoring='roc_auc',n_jobs=-1, verbose=True)
print(f'stacking classifier:{cv_results.mean()} ({cv_results.std()})')

stacked_models = []
stacked_models.append(('RFC1', RandomForestClassifier()))
stacked_models.append(('XGB1', XGBClassifier(early_stopping_rounds=10)))
stacked_models.append(('GBC1', GradientBoostingClassifier()))

from sklearn.metrics import accuracy_score
from sklearn.metrics import brier_score_loss

#Modelling on test data
test_models = models
test_models.append(('StackingClassifier', StackingClassifier(estimators=stacked_models, final_estimator=LogisticRegression())))


print(test_models)

results = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc','accuracy', 'brier_score'])
roc_plot_data = pd.DataFrame(columns=['classifier', 'fpr', 'tpr', 'auc'])
counter = 1
for name, model in test_models:
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    yproba = model.predict_proba(X_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, yproba)
    auc = roc_auc_score(y_test, yproba)
    accuracy = accuracy_score(y_test,y_pred)
    brier_score = brier_score_loss(y_test, yproba)
    results = results.append({'classifiers':name,
                              'fpr':fpr,
                              'tpr':tpr,
                              'auc':auc,
                              'accuracy':accuracy,
                              'brier_score':brier_score},ignore_index=True)
    roc_plot_data = roc_plot_data.append({'classifier':model.__class__.__name__,
                              'fpr':fpr,
                              'tpr':tpr,
                              'auc':auc,},ignore_index=True)
    print("iteration " + str(counter) + " " + name)
    counter += 1
    
results.set_index('classifiers', inplace=True)    
roc_plot_data.set_index('classifier', inplace=True)

# Description
roc_plot_data.head(15)
display(roc_plot_data)

import matplotlib.pyplot as plt


fig = plt.figure(figsize=(20,14))

# Plot ROC-AUC
for i in roc_plot_data.index:
    plt.plot(roc_plot_data.loc[i]['fpr'], 
             roc_plot_data.loc[i]['tpr'], 
             label="{}, AUC={:.3f}".format(i, roc_plot_data.loc[i]['auc']))
    
plt.plot([0,1], [0,1], color='orange', linestyle='--')

plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("False Positive Rate", fontsize=15)

plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.ylabel("True Positive Rate", fontsize=15)

plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)
plt.legend(prop={'size':13}, loc='lower right')

plt.show()

display(results)

# Hyperparamters for XGB Classifier
kf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)

model = XGBClassifier(early_stopping_rounds=10)

n_estimators = [100, 300, 400, 500, 700, 1000, 1250, 1500, 1750, 2000]
learning_rate = [0.0001, 0.001, 0.01, 0.02, 0.05, 0.1]
max_depth = [6, 10, 12, 15, 20]
subsample = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
colsample_bytree = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]


random_grid = {'n_estimators': n_estimators,
               'learning_rate': learning_rate,
               'max_depth': max_depth,
               'subsample': subsample,
               'colsample_bytree': colsample_bytree,
               }

random_search = RandomizedSearchCV(estimator = model,
                                   param_distributions = random_grid,
                                   n_iter = 10,
                                   cv = kf,
                                   scoring = 'f1_micro',
                                   n_jobs = -1,
                                   random_state=1,
                                   verbose=10
                                  )

random_search = random_search.fit(X_train, y_train)

print("Parameters of the best_estimator:")
print(random_search.best_params_)

# Hyperparamters for Random Forest Classifier
kf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)

model = RandomForestClassifier()

n_estimators = [100, 300, 400, 500, 700, 1000, 1250, 1500, 1750, 2000]
max_depth = [3, 6, 10, 15, 20]
min_samples_split = [2, 5, 10, 15, 20]
min_samples_leaf = [1, 2, 5, 10]
max_features = ['auto', 'sqrt', 'log2']

random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'max_features': max_features
              }

random_search = RandomizedSearchCV(estimator = model,
                                   param_distributions = random_grid,
                                   n_iter = 10,
                                   cv = kf,
                                   scoring = 'f1_micro',
                                   n_jobs = -1,
                                   random_state=1,
                                   verbose=10
                                  )

random_search = random_search.fit(X_train, y_train)

print("Parameters of the best_estimator:")
print(random_search.best_params_)

# Hyperparamters for Gradient Boosting Classifier
kf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)

model = GradientBoostingClassifier()

n_estimators = [100, 300, 400, 500, 700, 1000, 1250, 1500, 1750, 2000]
learning_rate = [0.0001, 0.001, 0.01, 0.02, 0.05, 0.1]
max_depth = [3, 6, 10, 15, 20]
subsample = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
max_features = ['auto', 'sqrt', 'log2']

random_grid = {'n_estimators': n_estimators,
               'learning_rate': learning_rate,
               'max_depth': max_depth,
               'subsample': subsample,
               'max_features': max_features
              }

random_search = RandomizedSearchCV(estimator = model,
                                   param_distributions = random_grid,
                                   n_iter = 10,
                                   cv = kf,
                                   scoring = 'f1_micro',
                                   n_jobs = -1,
                                   random_state=1,
                                   verbose=10
                                  )

random_search = random_search.fit(X_train, y_train)

print("Parameters of the best_estimator:")
print(random_search.best_params_)

models_hyper_parameters = []
models_hyper_parameters.append(('RFC', RandomForestClassifier(n_estimators=1000, min_samples_split=20, min_samples_leaf=2, max_features='log2', max_depth=20)))
models_hyper_parameters.append(('XGB', XGBClassifier(early_stopping_rounds=10, subsample=0.6, n_estimators=400, max_depth=6, learning_rate=0.01, colsample_bytree=0.8)))
models_hyper_parameters.append(('GBC', GradientBoostingClassifier(subsample=0.8, n_estimators=400, max_features='auto', max_depth=6, learning_rate=0.02)))


# # evaluate each model in turn
results = []
names = []
for name, model in models_hyper_parameters:
    kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)
    cv_results = cross_val_score(model, X_train, y_train.values.ravel(), cv=kfold, scoring='roc_auc',n_jobs=-1, verbose=True)
    results.append(cv_results)
    names.append(name)
    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
    
 
stacked_models_hyperparam = []
stacked_models_hyperparam.append(('RFC', RandomForestClassifier(n_estimators=1000, min_samples_split=20, min_samples_leaf=2, max_features='log2', max_depth=20)))
stacked_models_hyperparam.append(('XGB', XGBClassifier(early_stopping_rounds=10, subsample=0.6, n_estimators=400, max_depth=6, learning_rate=0.01, colsample_bytree=0.8)))
stacked_models_hyperparam.append(('GBC', GradientBoostingClassifier(subsample=0.8, n_estimators=400, max_features='auto', max_depth=6, learning_rate=0.02)))

from sklearn.metrics import accuracy_score
from sklearn.metrics import brier_score_loss

#Modelling on test data with hyper parameters
test_models_hyperparameters = models_hyper_parameters
test_models_hyperparameters.append(('StackingClassifier', StackingClassifier(estimators=stacked_models_hyperparam, final_estimator=LogisticRegression())))


results_hyper_param = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc','accuracy', 'brier_score'])
roc_plot_hyper_params = pd.DataFrame(columns=['classifier', 'fpr', 'tpr', 'auc'])
counter = 1
for name, model in test_models_2:
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    yproba = model.predict_proba(X_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, yproba)
    auc = roc_auc_score(y_test, yproba)
    accuracy = accuracy_score(y_test,y_pred)
    brier_score = brier_score_loss(y_test, yproba)
    results_hyper_param = results_hyper_param.append({'classifiers':name,
                              'fpr':fpr,
                              'tpr':tpr,
                              'auc':auc,
                              'accuracy':accuracy,
                              'brier_score':brier_score},ignore_index=True)
    roc_plot_hyper_params = roc_plot_hyper_params.append({'classifier':model.__class__.__name__,
                              'fpr':fpr,
                              'tpr':tpr,
                              'auc':auc,},ignore_index=True)
    print("iteration " + str(counter) + " " + name)
    counter += 1
    
results_hyper_param.set_index('classifiers', inplace=True)    
roc_plot_hyper_params.set_index('classifier', inplace=True)


# Plot ROC-AUC Curve
fig = plt.figure(figsize=(20,14))


for i in roc_plot_hyper_params.index:
    plt.plot(roc_plot_hyper_params.loc[i]['fpr'], 
             roc_plot_hyper_params.loc[i]['tpr'], 
             label="{}, AUC={:.3f}".format(i, roc_plot_hyper_params.loc[i]['auc']))
    
plt.plot([0,1], [0,1], color='orange', linestyle='--')

plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("False Positive Rate", fontsize=15)

plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.ylabel("True Positive Rate", fontsize=15)

plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)
plt.legend(prop={'size':13}, loc='lower right')

plt.show()

# Display metrics
display(results_hyper_param)
